{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e626008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from scipy.io import loadmat\n",
    "from preprocess_methods import CausalIndividualLevel\n",
    "from pathlib import Path\n",
    "from causal_discovery.algos.notears import NoTears\n",
    "import networkx as nx\n",
    "import hashlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e9e7c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96f63e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GS3 = sio.loadmat('../../Data/imbalanced performance/Full factorial design of experiments dataset for parallel-connected lithium-ion cells imbalanced performance investigation/Parallel-connected module experimental campaign/1_Single_cell_characterisation/Aged_cells/HPPC_MS/HPPC_MultiSine_GS3.mat', struct_as_record=False, simplify_cells=True)\n",
    "Y1= sio.loadmat('../../Data/imbalanced performance/Full factorial design of experiments dataset for parallel-connected lithium-ion cells imbalanced performance investigation/Parallel-connected module experimental campaign/1_Single_cell_characterisation/Aged_cells/HPPC_MS/HPPC_MultiSine_Y1.mat', struct_as_record=False, simplify_cells=True)\n",
    "cells = [['GS3)', GS3], ['Y1' , Y1]]\n",
    "\n",
    "OGS3 = sio.loadmat('../../Data/imbalanced performance/Full factorial design of experiments dataset for parallel-connected lithium-ion cells imbalanced performance investigation/Parallel-connected module experimental campaign/1_Single_cell_characterisation/Aged_cells/Pseudo_OCV/OCVDis_GS3.mat', struct_as_record=False, simplify_cells=True)\n",
    "OY1 = sio.loadmat('../../Data/imbalanced performance/Full factorial design of experiments dataset for parallel-connected lithium-ion cells imbalanced performance investigation/Parallel-connected module experimental campaign/1_Single_cell_characterisation/Aged_cells/Pseudo_OCV/OCVDis_Y1.mat', struct_as_record=False, simplify_cells=True)\n",
    "#cells = [['OGS3', OGS3], ['OY1' , OY1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21e0c7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f'P{i}' for i in range(2, 21)]        # gives ['P2', 'P3', ..., 'P20']\n",
    "\n",
    "base_path = '../../Data/imbalanced performance/Full factorial design of experiments dataset for parallel-connected lithium-ion cells imbalanced performance investigation/Parallel-connected module experimental campaign/1_Single_cell_characterisation/NMC_cells/HPPC_MS/HPPC_MultiSine_'\n",
    "for p in files:\n",
    "    new_data = sio.loadmat(base_path + f'{p}.mat', struct_as_record=False, simplify_cells=True)\n",
    "    cells.append([str(p), new_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0967b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f'F{i}' for i in range(1, 19)]        # gives ['F1', 'F2', ..., 'F18']\n",
    "\n",
    "base_path = '../../Data/imbalanced performance/Full factorial design of experiments dataset for parallel-connected lithium-ion cells imbalanced performance investigation/Parallel-connected module experimental campaign/1_Single_cell_characterisation/NCA_cells/HPPC_MS/HPPC_MultiSine_'\n",
    "for f in files:\n",
    "    new_data = sio.loadmat(base_path + f'{f}.mat', struct_as_record=False, simplify_cells=True)\n",
    "    cells.append([str(f), new_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "733d90a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         CurrentData  \\\n",
      "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "\n",
      "                                         VoltageData  \\\n",
      "0  [4.197761058807373, 4.197635173797607, 4.19782...   \n",
      "1  [4.198263168334961, 4.1982808113098145, 4.1982...   \n",
      "2  [4.1984028816223145, 4.198349475860596, 4.1983...   \n",
      "3  [4.19843864440918, 4.198358058929443, 4.198379...   \n",
      "4  [4.198364734649658, 4.1983642578125, 4.1983065...   \n",
      "\n",
      "                                            TempData  \\\n",
      "0  [24.041702270507812, 23.95568084716797, 23.926...   \n",
      "1  [23.43080711364746, 23.43080711364746, 23.4161...   \n",
      "2  [22.298236846923828, 22.298236846923828, 22.29...   \n",
      "3  [23.2643928553498, 23.26439666748047, 23.26439...   \n",
      "4  [22.531064987182617, 22.54578399658203, 22.545...   \n",
      "\n",
      "                                          Step_Index  \\\n",
      "0  [9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, ...   \n",
      "1  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...   \n",
      "2  [9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, ...   \n",
      "3  [9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, ...   \n",
      "4  [9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, ...   \n",
      "\n",
      "                                          CycleIndex  \\\n",
      "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "\n",
      "                                            TimeData  \n",
      "0  [0.0, 1.0003000000142492, 2.0001000000047497, ...  \n",
      "1  [1.0014, 2.0011999999987893, 3.001100000001315...  \n",
      "2  [0.0, 1.000899999999092, 2.0002999999996973, 2...  \n",
      "3  [0.0, 1.0000999999901978, 2.000799999994342, 3...  \n",
      "4  [0.0, 1.0005999999993946, 2.0010000000038417, ...  \n"
     ]
    }
   ],
   "source": [
    "cell_data = []\n",
    "for cell in cells:\n",
    "    cell_info = {\n",
    "        'CurrentData': cell[1]['CurrentData'], \n",
    "        'VoltageData': cell[1]['VoltageData'],\n",
    "        'TempData': cell[1]['TempData'],\n",
    "        'Step_Index': cell[1]['StepIndex'],\n",
    "        'CycleIndex' : cell[1]['CycleIndex'],\n",
    "        'TimeData' : cell[1]['TimeData']\n",
    "    }\n",
    "    cell_data.append(cell_info)\n",
    "\n",
    "cell_dataset = pd.DataFrame(cell_data)\n",
    "print(cell_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93a31924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_array(arr):\n",
    "    # Works for arrays like [[x], [y], [z]]\n",
    "    return np.array([x[0] if isinstance(x, (list, np.ndarray)) else x for x in arr])\n",
    "\n",
    "def make_cell_dataframe(row, cols):\n",
    "    clean = {}\n",
    "\n",
    "    # Flatten all nested arrays\n",
    "    for name in cols:\n",
    "        clean[name] = flatten_array(row[name])\n",
    "\n",
    "    # Build long-format dataframe\n",
    "    return pd.DataFrame(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e54dcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataForEachByStep(data, cols, groups):\n",
    "    cell_list = []\n",
    "    for idx, row in data.iterrows():\n",
    "        #cell_info = {name : row[name] for name in cols}\n",
    "        #new_pd = pd.DataFrame([cell_info])\n",
    "        cell_df = make_cell_dataframe(row, cols)\n",
    "        obs_datasets = []\n",
    "        interventional_datasets = []\n",
    "        for i in range(len(groups)):\n",
    "            subset = cell_df[cell_df[\"Step_Index\"].isin(groups[i])].reset_index(drop=True)\n",
    "\n",
    "            if i == 0:\n",
    "                obs_datasets.append(subset)\n",
    "\n",
    "            interventional_datasets.append(subset)\n",
    "        cell_list.append([obs_datasets, interventional_datasets])\n",
    "    return cell_list\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc3ee3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Step_Index   TempData  VoltageData  CurrentData\n",
      "0              11  23.226440     3.658872   -18.192619\n",
      "1              11  23.226440     3.658149   -17.971931\n",
      "2              11  23.226440     3.656804   -17.875931\n",
      "3              11  23.226440     3.652112   -17.928795\n",
      "4              11  23.226440     3.644859   -18.089554\n",
      "...           ...        ...          ...          ...\n",
      "17610          18  23.357729     3.168857     0.000000\n",
      "17611          18  23.357729     3.168854     0.000000\n",
      "17612          18  23.320255     3.168962     0.000000\n",
      "17613          18  23.320255     3.169020     0.000000\n",
      "17614          18  23.320255     3.168992     0.000000\n",
      "\n",
      "[17615 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "groups = [[11, 13, 16, 18, 21], [9, 14], [10, 16], [12], [17], [19], [20], [22]]\n",
    "cols = ['Step_Index', 'TempData', 'VoltageData', 'CurrentData']\n",
    "sub_df_1 = splitDataForEachByStep(cell_dataset, cols, groups)\n",
    "print(sub_df_1[1][1][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c946dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          TempData  VoltageData  CurrentData  Group\n",
      "0        24.041702     4.197761          0.0    1.0\n",
      "1        23.955681     4.197635          0.0    1.0\n",
      "2        23.926302     4.197824          0.0    1.0\n",
      "3        23.979719     4.197635          0.0    1.0\n",
      "4        24.001940     4.197635          0.0    1.0\n",
      "...            ...          ...          ...    ...\n",
      "2788247  22.857420     3.319781          0.0    7.0\n",
      "2788248  22.857420     3.319725          0.0    7.0\n",
      "2788249  22.857420     3.319683          0.0    7.0\n",
      "2788250  22.857420     3.319831          0.0    7.0\n",
      "2788251  22.857420     3.319834          0.0    7.0\n",
      "\n",
      "[2788252 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "multicausaldataset = CausalIndividualLevel.multienvcausaldiscoverydataset(cell_dataset, cols, groups)\n",
    "print(multicausaldataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9f10d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CausalIndividualLevel.multienvcausaldiscovery(multicausaldataset, '../../Src/structures/1cells_char/images/multienv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ff6e55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf686e54d644b3cad661db3f5ebc08c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned adjacency matrix:\n",
      " [[nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]]\n",
      "Adjacency matrix:\n",
      "[[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "CausalIndividualLevel.multienvcausaldiscoveryChat(multicausaldataset, '../../Src/structures/1cells_char/images/multienv/chat_dagma')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22405f04",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a5c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CausalIndividualLevel.multienvcausaldiscoverypergroup(multicausaldataset, '../../Src/structures/1cells_char/images/multienv/per_group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2129450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaled_cells = []\n",
    "for cell in cells:\n",
    "    CurrentData = np.asarray(cell[1][\"CurrentData\"]).flatten()\n",
    "    VoltageData = np.asarray(cell[1][\"VoltageData\"]).flatten()\n",
    "    StepIndex    = np.asarray(cell[1][\"StepIndex\"]).flatten()\n",
    "    TempData    = np.asarray(cell[1][\"TempData\"]).flatten()\n",
    "    df = pd.DataFrame({\n",
    "        \"Voltage_Data\": VoltageData,\n",
    "        \"Current_Data\": CurrentData,\n",
    "        \"Step_Index\": StepIndex,\n",
    "        \"Temp_Data\": TempData,\n",
    "    })\n",
    "    scaler = StandardScaler()\n",
    "    scaled = scaler.fit_transform(df.values)\n",
    "    scaled_df = pd.DataFrame(scaled, columns=df.columns)\n",
    "    scaled_cells.append(scaled_df)\n",
    "\n",
    "print(scaled_cells)\n",
    "#cell_dataset = pd.DataFrame(cell_data)\n",
    "#print(cell_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55c7540",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_concat = pd.concat(scaled_cells, ignore_index=True)\n",
    "CausalIndividualLevel.run_bigdata_notears(pd_concat, '../../Src/structures/1cells_char/images/multienv/chat_with_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b07b7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def observational_struct_learning(cell_list, obs_or_int, save_point, sort_by=\"Step_Index\", type=\"cell\"):\n",
    "    os.makedirs(save_point, exist_ok=True)\n",
    "\n",
    "    dag_counter = {}  # counts identical DAGs\n",
    "\n",
    "    for idx, cell in enumerate(cell_list):\n",
    "\n",
    "        # Create folder for this cell\n",
    "        cell_dir = os.path.join(save_point, f\"{type}_{idx}\")\n",
    "        os.makedirs(cell_dir, exist_ok=True)\n",
    "\n",
    "        for group_idx, group in enumerate(cell[obs_or_int]):\n",
    "\n",
    "            df_cell = group.copy()\n",
    "\n",
    "            # ---- skip empty ----\n",
    "            if df_cell.empty:\n",
    "                continue\n",
    "\n",
    "            # ---- drop Step_Index ----\n",
    "            if sort_by in df_cell.columns:\n",
    "                df_cell = df_cell.drop(columns=[sort_by])\n",
    "\n",
    "            # ---- drop zero-variance columns ----\n",
    "            df_cell = df_cell.loc[:, df_cell.nunique() > 1]\n",
    "            if df_cell.shape[1] < 2:\n",
    "                continue\n",
    "\n",
    "            # Prepare data\n",
    "            X = df_cell.to_numpy().astype(float)\n",
    "            cols = df_cell.columns.tolist()\n",
    "\n",
    "            # ---- run NoTears ----\n",
    "            no_tears_alg = NoTears(rho=1, alpha=0.1, l1_reg=0, lr=1e-2)\n",
    "            no_tears_alg.learn(X)\n",
    "\n",
    "            W_est = no_tears_alg.get_result()\n",
    "            dag = (np.abs(W_est) > 0.3).astype(int)\n",
    "\n",
    "            # ---- count DAG ----\n",
    "            dag_hash = hashlib.md5(dag.tobytes()).hexdigest()\n",
    "            dag_counter[dag_hash] = dag_counter.get(dag_hash, 0) + 1\n",
    "\n",
    "            # ---- create folder for this group ----\n",
    "            group_dir = os.path.join(cell_dir, f\"group_{group_idx}\")\n",
    "            os.makedirs(group_dir, exist_ok=True)\n",
    "\n",
    "            # ---- save adjacency matrix ----\n",
    "            mat_path = os.path.join(group_dir, \"dag.npy\")\n",
    "            np.save(mat_path, dag)\n",
    "\n",
    "            # ---- draw DAG ----\n",
    "            G = nx.DiGraph()\n",
    "            for i, src in enumerate(cols):\n",
    "                for j, tgt in enumerate(cols):\n",
    "                    if dag[i, j] == 1:\n",
    "                        G.add_edge(src, tgt)\n",
    "\n",
    "            plt.figure(figsize=(7, 6))\n",
    "            pos = nx.spring_layout(G, seed=42)\n",
    "            nx.draw(G, pos, with_labels=True, node_size=2000, font_size=10, arrows=True)\n",
    "            plt.title(f\"Cell {idx} – Group {group_idx} – Learned DAG\")\n",
    "\n",
    "            img_path = os.path.join(group_dir, \"dag.png\")\n",
    "            plt.savefig(img_path, dpi=200, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "\n",
    "            print(f\"Saved DAG → cell {idx} / group {group_idx}\")\n",
    "\n",
    "    # ---- save summary at the end only ----\n",
    "    summary_path = os.path.join(save_point, \"dag_summary.txt\")\n",
    "    with open(summary_path, \"w\") as f:\n",
    "        for h, count in dag_counter.items():\n",
    "            f.write(f\"{h}: {count}\\n\")\n",
    "\n",
    "    print(\"\\nDAG summary saved to:\", summary_path)\n",
    "    print(\"Unique DAGs found:\", len(dag_counter))\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc216c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_point = '../../Src/structures/1cells_char/images/obs'\n",
    "observational_struct_learning(sub_df_1, 0, save_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10fde96",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_point = '../../Src/structures/1cells_char/images/int'\n",
    "observational_struct_learning(sub_df_1, 1, save_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf94e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_visualize_unique_dags(save_point):\n",
    "    \"\"\"\n",
    "    Reads dag_summary.txt, reconstructs unique DAGs from stored dag.npy files,\n",
    "    and shows them visually with their counts.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- Load summary ----\n",
    "    summary_path = os.path.join(save_point, \"dag_summary.txt\")\n",
    "    if not os.path.exists(summary_path):\n",
    "        print(\"ERROR: No dag_summary.txt found in:\", save_point)\n",
    "        return\n",
    "\n",
    "    summary = {}\n",
    "    with open(summary_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            h, count = line.strip().split(\": \")\n",
    "            summary[h] = int(count)\n",
    "\n",
    "    unique_dags = {}\n",
    "\n",
    "    # ---- Traverse folder structure ----\n",
    "    for cell in os.listdir(save_point):\n",
    "        cell_path = os.path.join(save_point, cell)\n",
    "        if not os.path.isdir(cell_path):\n",
    "            continue\n",
    "\n",
    "        for group in os.listdir(cell_path):\n",
    "            group_path = os.path.join(cell_path, group)\n",
    "            dag_path = os.path.join(group_path, \"dag.npy\")\n",
    "\n",
    "            if not os.path.exists(dag_path):\n",
    "                continue\n",
    "\n",
    "            dag = np.load(dag_path)\n",
    "            dag_hash = hashlib.md5(dag.tobytes()).hexdigest()\n",
    "\n",
    "            # Only keep DAGs mentioned in the summary\n",
    "            if dag_hash in summary:\n",
    "                unique_dags[dag_hash] = dag\n",
    "\n",
    "    # ---- Visualization ----\n",
    "    print(\"Found\", len(unique_dags), \"unique DAGs.\")\n",
    "\n",
    "    n = len(unique_dags)\n",
    "    plt.figure(figsize=(6 * n, 6))\n",
    "\n",
    "    for i, (dag_hash, dag) in enumerate(unique_dags.items(), 1):\n",
    "        G = nx.DiGraph()\n",
    "        num_nodes = dag.shape[0]\n",
    "        nodes = [f\"X{i}\" for i in range(num_nodes)]\n",
    "        G.add_nodes_from(nodes)\n",
    "\n",
    "        for s in range(num_nodes):\n",
    "            for t in range(num_nodes):\n",
    "                if dag[s, t] == 1:\n",
    "                    G.add_edge(nodes[s], nodes[t])\n",
    "\n",
    "        plt.subplot(1, n, i)\n",
    "        pos = nx.spring_layout(G, seed=0)\n",
    "        nx.draw(\n",
    "            G,\n",
    "            pos,\n",
    "            with_labels=True,\n",
    "            node_size=2000,\n",
    "            font_size=10,\n",
    "            arrows=True\n",
    "        )\n",
    "        plt.title(f\"DAG {i}\\nHash: {dag_hash[:6]}\\nCount: {summary[dag_hash]}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return unique_dags, summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa19a63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_point_obs = '../../Src/structures/1cells_char/images/obs'\n",
    "obs_unique_dags, obs_summary = load_and_visualize_unique_dags(save_point_obs)\n",
    "#x0 = temprature, x1 = voltage, x2 = current (amper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871afa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_point_int = '../../Src/structures/1cells_char/images/int'\n",
    "int_unique_dags, int_summary = load_and_visualize_unique_dags(save_point_int)\n",
    "#x0 = temprature, x1 = voltage, x2 = current (amper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132951ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: real variable names from preprocessing step\n",
    "node_names = [\"TempData\", \"Voltage\", \"Current\"] # <-- IMPORTANT\n",
    "\n",
    "edge_counts = CausalIndividualLevel.count_edge_frequencies(int_unique_dags, int_summary, node_names)\n",
    "\n",
    "for edge, count in sorted(edge_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{edge[0]} → {edge[1]}: {count} occurrences\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
